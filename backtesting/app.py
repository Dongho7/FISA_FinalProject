# app.py (í•˜ë½ì¥ ìŒì˜ ë¶„ì„ ê¸°ëŠ¥ ì¶”ê°€)

import pandas as pd
import numpy as np
from flask import Flask, jsonify, request, Response
from flask_cors import CORS
from datetime import datetime
import os
import json 
import google.generativeai as genai
import time
from dotenv import load_dotenv

load_dotenv()

gemini_api_key = os.getenv("GEMINI_API_KEY")  # â­ï¸ (í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ)

# --- 0. AI ì„¤ì • (Gemini) ---
try:
    genai.configure(api_key=gemini_api_key)
    model = genai.GenerativeModel('models/gemini-flash-latest') 
except Exception as e:
    print(f"âš ï¸ AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}. API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    model = None

g_ai_prompt_cache = None
g_backtest_result_cache = None # â­ï¸ ë°˜ë³µ ê³„ì‚° ë°©ì§€ ìºì‹œ

# --- 1. ë°±í…ŒìŠ¤íŒ… í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼) ---

def create_daily_base_rate_series(start_date, end_date):
    # (ì´ì „ê³¼ ë™ì¼... ê¸°ì¤€ê¸ˆë¦¬ ì´ë ¥)
    rate_history = {
        '2021-11-25': 1.00, '2022-01-14': 1.25, '2022-04-14': 1.50,
        '2022-05-26': 1.75, '2022-07-13': 2.25, '2022-08-25': 2.50,
        '2022-10-12': 3.00, '2022-11-24': 3.25, '2023-01-13': 3.50,
        '2024-10-11': 3.25, '2024-11-28': 3.00, '2025-02-25': 2.75,
        '2025-05-29': 2.50,
    }
    all_days = pd.date_range(start=start_date, end=end_date, freq='D')
    rate_series = pd.Series(index=all_days, name="base_rate")
    for date_str, rate in rate_history.items():
        rate_series.loc[rate_series.index >= pd.to_datetime(date_str)] = rate
    rate_series.ffill(inplace=True) 
    return rate_series / 100.0

def load_data(etf_file, kospi_file, start_date, end_date):
    # (ì´ì „ê³¼ ë™ì¼... CSV ë¡œë“œ)
    try:
        df_etf = pd.read_csv(etf_file, index_col='Date', parse_dates=True)
        df_kospi = pd.read_csv(kospi_file, index_col='Date', parse_dates=True)
        assets_to_use = ['226490', '114260', '363570']
        df_etf = df_etf[assets_to_use]
        if 'KOSPI' not in df_kospi.columns:
            df_kospi.rename(columns={df_kospi.columns[0]: 'KOSPI'}, inplace=True)
        price_df = pd.concat([df_etf, df_kospi['KOSPI']], axis=1)
        daily_rate_series = create_daily_base_rate_series(start_date, end_date)
        price_df = price_df.join(daily_rate_series)
        price_df.ffill(inplace=True); price_df.bfill(inplace=True)
        price_df.rename(columns={'KOSPI': 'benchmark'}, inplace=True)
        return price_df
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}"); return None

def run_monthly_rebalancing_backtest(price_df, initial_capital, target_weights, assets_by_group):
    # (ì´ì „ê³¼ ë™ì¼... ì›”ê°„ ë¦¬ë°¸ëŸ°ì‹± ì‹¤í–‰)
    dates = price_df.index
    asset_keys = assets_by_group['Stocks'] + assets_by_group['Bonds']
    cash_weight = target_weights['Cash']
    current_shares = {asset: 0 for asset in asset_keys}
    current_cash_value = 0.0
    portfolio_history = [] 
    first_date = dates[0]
    first_prices = price_df.loc[first_date]
    current_cash_value = initial_capital * cash_weight
    stock_value = 0.0; bond_value = 0.0
    for asset in assets_by_group['Stocks']:
        target_value = initial_capital * target_weights[asset]
        current_shares[asset] = target_value / first_prices[asset]
        stock_value += target_value
    for asset in assets_by_group['Bonds']:
        target_value = initial_capital * target_weights[asset]
        current_shares[asset] = target_value / first_prices[asset]
        bond_value += target_value
    portfolio_history.append({'date': first_date.strftime('%Y-%m-%d'), 'value': initial_capital, 'stock_value': stock_value, 'bond_value': bond_value, 'cash_value': current_cash_value})
    for i in range(1, len(dates)):
        date = dates[i]
        today_prices = price_df.loc[date]
        daily_rate = price_df.loc[date, 'base_rate'] / 365.0
        current_cash_value *= (1 + daily_rate)
        stock_value = 0.0; bond_value = 0.0
        for asset in assets_by_group['Stocks']:
            stock_value += current_shares[asset] * today_prices[asset]
        for asset in assets_by_group['Bonds']:
            bond_value += current_shares[asset] * today_prices[asset]
        current_total_value = stock_value + bond_value + current_cash_value
        portfolio_history.append({'date': date.strftime('%Y-%m-%d'), 'value': current_total_value, 'stock_value': stock_value, 'bond_value': bond_value, 'cash_value': current_cash_value})
        is_rebalancing_day = (date.month != (date + pd.Timedelta(days=1)).month)
        if is_rebalancing_day:
            current_cash_value = current_total_value * cash_weight
            for asset in asset_keys:
                target_value = current_total_value * target_weights[asset]
                current_shares[asset] = target_value / today_prices[asset]
    return portfolio_history

def calculate_stats(series):
    # (ì´ì „ê³¼ ë™ì¼... CAGR, MDD ê³„ì‚°)
    end_val = series.iloc[-1]; start_val = series.iloc[0]
    num_years = (series.index[-1] - series.index[0]).days / 365.25
    cagr = (end_val / start_val) ** (1 / num_years) - 1
    peak = series.cummax(); drawdown = (series - peak) / peak; mdd = drawdown.min()
    return {"CAGR": cagr, "MDD": mdd, "Final Value": end_val}

# --- 2. í¬íŠ¸í´ë¦¬ì˜¤ ì„¤ì • ---
ETF_FILE = "ETF_20191201_20251107.csv"
KOSPI_FILE = "KOSPI_20191201_to_20251107.csv"
START_DATE = "2019-12-01"; END_DATE = "2025-11-07"
STOCKS = ['226490']; BONDS = ['114260', '363570']
ASSETS_BY_GROUP = {'Stocks': STOCKS, 'Bonds': BONDS}
TARGET_WEIGHTS = {'226490': 0.60, '114260': 0.15, '363570': 0.15, 'Cash': 0.10 }
INITIAL_CAPITAL = 100_000_000

# â­ï¸ [ì‹ ê·œ] ì‚¬ìš©ìê°€ ìš”ì²­í•œ í•˜ë½ì¥ ìŒì˜ êµ¬ê°„
CRASH_PERIODS = [
    {"name": "ì½”ë¡œë‚˜19 ê¸‰ë½ì¥", "start": "2020-02-14", "end": "2020-03-19"},
    {"name": "2024ë…„ 8ì›” í•˜ë½(ì‹œìŠ¤í…œ ë¦¬ìŠ¤í¬ ë°œìƒ)", "start": "2024-07-30", "end": "2024-08-05"},
    {"name": "2025ë…„ 4ì›” í•˜ë½(ê´€ì„¸ ì´ìŠˆ)", "start": "2025-03-26", "end": "2025-04-09"},
]

# --- 3. â­ï¸ AI ë¶„ì„ í•¨ìˆ˜ (ìˆ˜ì •) ---

def find_analysis_data(portfolio_history, benchmark_history, user_crash_periods):
    """(ìˆ˜ì •) ë²¤ì¹˜ë§ˆí¬ì˜ (1)ìµœì•…ì˜ í•˜ë½ 'ê¸°ê°„', (2)ìµœì•…ì˜ 'í•˜ë£¨', (3)ì‚¬ìš©ì ì§€ì • ê¸°ê°„ì„ ëª¨ë‘ ë¶„ì„í•©ë‹ˆë‹¤."""
    
    # 1. ì‹œê³„ì—´ ë°ì´í„°ë¡œ ë³€í™˜
    bm_series = pd.Series({pd.to_datetime(item['date']): item['value'] for item in benchmark_history})
    pf_df = pd.DataFrame(portfolio_history); pf_df['date'] = pd.to_datetime(pf_df['date']); pf_df = pf_df.set_index('date'); pf_df.index.name = 'Date'
    
    # 2. ë²¤ì¹˜ë§ˆí¬ì˜ MDD (ìµœì•…ì˜ 'ê¸°ê°„') ë¶„ì„
    bm_peak = bm_series.cummax(); bm_drawdown = (bm_series - bm_peak) / bm_peak
    mdd_trough_date = bm_drawdown.idxmin(); mdd_peak_date = bm_series.loc[:mdd_trough_date].idxmax()
    
    def get_period_return(series, start_date_str, end_date_str):
        try:
            # â­ï¸ 'asof'ëŠ” í•´ë‹¹ ë‚ ì§œ í˜¹ì€ ê·¸ ì´ì „ì— ê°€ì¥ ê°€ê¹Œìš´ ë‚ ì§œì˜ ë°ì´í„°ë¥¼ ì°¾ìŒ (íœ´ì¥ì¼ ëŒ€ì‘)
            start_date = pd.to_datetime(start_date_str)
            end_date = pd.to_datetime(end_date_str)
            
            # â­ï¸ ë°ì´í„° ë²”ìœ„ ë°–ì˜ ê¸°ê°„ì„ ìš”ì²­í•  ê²½ìš° np.nan ë°˜í™˜
            if end_date < series.index.min() or start_date > series.index.max():
                return np.nan
                
            start_val = series.asof(start_date)
            end_val = series.asof(end_date)
            
            if pd.isna(start_val) or pd.isna(end_val): return np.nan
            return (end_val / start_val) - 1
        except Exception:
            return np.nan # ë‚ ì§œ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ë“± ì˜ˆì™¸ ë°œìƒ ì‹œ

    mdd_period_analysis = {
        "start_date": mdd_peak_date.strftime('%Y-%m-%d'), "end_date": mdd_trough_date.strftime('%Y-%m-%d'),
        "benchmark_return": get_period_return(bm_series, mdd_peak_date, mdd_trough_date),
        "portfolio_return": get_period_return(pf_df['value'], mdd_peak_date, mdd_trough_date),
        "stock_return": get_period_return(pf_df['stock_value'], mdd_peak_date, mdd_trough_date),
        "bond_return": get_period_return(pf_df['bond_value'], mdd_peak_date, mdd_trough_date),
        "cash_return": get_period_return(pf_df['cash_value'], mdd_peak_date, mdd_trough_date)
    }
    
    # 3. ë²¤ì¹˜ë§ˆí¬ì˜ ìµœì•…ì˜ 'í•˜ë£¨' ë¶„ì„
    bm_daily_returns = bm_series.pct_change(); pf_daily_returns = pf_df['value'].pct_change()
    worst_day_date = bm_daily_returns.idxmin()
    worst_day_analysis = {
        "date": worst_day_date.strftime('%Y-%m-%d'),
        "benchmark_return": bm_daily_returns.loc[worst_day_date],
        "portfolio_return": pf_daily_returns.loc[worst_day_date]
    }
    
    # --- 4. â­ï¸ [ì‹ ê·œ] ì‚¬ìš©ìê°€ ì •ì˜í•œ í•˜ë½ì¥ ë¶„ì„ ---
    user_period_analyses = []
    for period in user_crash_periods:
        start_date = period['start']
        end_date = period['end']
        
        analysis = {
            "name": period['name'],
            "start_date": start_date,
            "end_date": end_date,
            "benchmark_return": get_period_return(bm_series, start_date, end_date),
            "portfolio_return": get_period_return(pf_df['value'], start_date, end_date),
            "stock_return": get_period_return(pf_df['stock_value'], start_date, end_date),
            "bond_return": get_period_return(pf_df['bond_value'], start_date, end_date),
            "cash_return": get_period_return(pf_df['cash_value'], start_date, end_date),
        }
        # â­ï¸ ê³„ì‚°ëœ ê²½ìš°ì—ë§Œ(NaNì´ ì•„ë‹˜) ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        if not pd.isna(analysis['benchmark_return']):
            user_period_analyses.append(analysis)

    return mdd_period_analysis, worst_day_analysis, user_period_analyses


def generate_ai_analysis_prompt(stats, mdd_period_analysis, worst_day_analysis, user_period_analyses, user_weights):
    """(ìˆ˜ì •) AIì—ê²Œ ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
    
    weights_str = f"ì£¼ì‹ {user_weights['226490']*100:.0f}%, ì±„ê¶Œ {(user_weights['114260'] + user_weights['363570'])*100:.0f}%, í˜„ê¸ˆ {user_weights['Cash']*100:.0f}%"
    
    # â­ï¸ [ì‹ ê·œ] ì‚¬ìš©ì ì§€ì • í•˜ë½ì¥ ë¶„ì„ ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
    user_periods_str = "\n"
    for analysis in user_period_analyses:
        user_periods_str += (
            f"    - **{analysis['name']} ({analysis['start_date']} ~ {analysis['end_date']})**\n"
            f"      - KOSPI: {analysis['benchmark_return']:.2%}\n"
            f"      - í¬íŠ¸í´ë¦¬ì˜¤: {analysis['portfolio_return']:.2%}\n"
            f"      (ë‹¹ì‹œ ë‚´ë¶€ ì„±ê³¼: ì£¼ì‹ {analysis['stock_return']:.2%}, ì±„ê¶Œ {analysis['bond_return']:.2%}, í˜„ê¸ˆ {analysis['cash_return']:.2%})\n"
        )

    prompt = f"""
    ë‹¹ì‹ ì€ ì „ë¬¸ ìì‚° ê´€ë¦¬ ì–´ë“œë°”ì´ì €ì…ë‹ˆë‹¤. ì‚¬ìš©ì('ì‚¬ìš©ì'ë¼ê³  ë¶ˆëŸ¬ì¤˜)ì˜ ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ì¹œì ˆí•œ ì¡°ì–¸ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.(ë‹¨ 10ì¤„ ì´ë‚´ë¡œ í•´ì£¼ì‹œê³ , ë‚´ìš©ë³„ë¡œ ë‹¨ë½ ë„ì–´ì“°ê¸°ë¥¼ í•´ì£¼ì„¸ìš”) ì²« ì¸ì‚¬ëŠ” ì´ë ‡ê²Œ í•´ì£¼ì„¸ìš”, "ì•ˆë…•í•˜ì„¸ìš”! ì „ë¬¸ ìì‚° ê´€ë¦¬ ì–´ë“œë°”ì´ì €ì…ë‹ˆë‹¤."

    [1. ì‚¬ìš©ìì˜ í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„±]
    - {weights_str}
    - ë²¤ì¹˜ë§ˆí¬: KOSPI
    - í…ŒìŠ¤íŠ¸ ê¸°ê°„: {START_DATE} ~ {END_DATE}

    [2. ìµœì¢… ì„±ê³¼ ìš”ì•½]
    - ë‚˜ì˜ í¬íŠ¸í´ë¦¬ì˜¤:
        - ìµœì¢… ìì‚°: {stats['portfolio']['Final Value']:,.0f} ì›
        - ì—°í‰ê·  ìˆ˜ìµë¥  (CAGR): {stats['portfolio']['CAGR']:.2%}
        - ìµœëŒ€ ì†ì‹¤í­ (MDD): {stats['portfolio']['MDD']:.2%}
    - KOSPI (ë²¤ì¹˜ë§ˆí¬):
        - ìµœì¢… ìì‚°: {stats['benchmark']['Final Value']:,.0f} ì›
        - ì—°í‰ê·  ìˆ˜ìµë¥  (CAGR): {stats['benchmark']['CAGR']:.2%}
        - ìµœëŒ€ ì†ì‹¤í­ (MDD): {stats['benchmark']['MDD']:.2%}

    [3. KOSPI ìµœì•…ì˜ í•˜ë½ 'ê¸°ê°„' ë¶„ì„ ({mdd_period_analysis['start_date']} ~ {mdd_period_analysis['end_date']})]
    - ì´ ê¸°ê°„ KOSPIëŠ” {mdd_period_analysis['benchmark_return']:.2%} í•˜ë½í–ˆìŠµë‹ˆë‹¤.
    - ê°™ì€ ê¸°ê°„, 'ë‚˜ì˜ í¬íŠ¸í´ë¦¬ì˜¤'ëŠ” {mdd_period_analysis['portfolio_return']:.2%} í•˜ë½ìœ¼ë¡œ ë°©ì–´í–ˆìŠµë‹ˆë‹¤.
    - (ë‹¹ì‹œ í¬íŠ¸í´ë¦¬ì˜¤ ë‚´ë¶€: ì£¼ì‹ {mdd_period_analysis['stock_return']:.2%}, ì±„ê¶Œ {mdd_period_analysis['bond_return']:.2%}, í˜„ê¸ˆ {mdd_period_analysis['cash_return']:.2%})

    [4. KOSPI ìµœì•…ì˜ 'í•˜ë£¨' ë¶„ì„ ({worst_day_analysis['date']})]
    - ì´ ë‚  KOSPIëŠ” í•˜ë£¨ ë§Œì— {worst_day_analysis['benchmark_return']:.2%} ê¸‰ë½í–ˆìŠµë‹ˆë‹¤.
    - ê°™ì€ ë‚ , 'ë‚˜ì˜ í¬íŠ¸í´ë¦¬ì˜¤'ëŠ” {worst_day_analysis['portfolio_return']:.2%} í•˜ë½í–ˆìŠµë‹ˆë‹¤.

    [5. â­ï¸ ì‚¬ìš©ì ì§€ì • í•˜ë½ì¥ ë¶„ì„]
    {user_periods_str}

    [ì§€ì‹œì‚¬í•­]
    ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë‹¤ìŒ 5ê°€ì§€ í•­ëª©ì„ í¬í•¨í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. (ê°•ì¡°ë¥¼ ìœ„í•´ ** ì‚¬ìš© ê¸ˆì§€)

    1.  ìš©ì–´ ì„¤ëª…: "ìµœëŒ€ ì†ì‹¤í­(MDD)"ê³¼ "ì—°í‰ê·  ìˆ˜ìµë¥ (CAGR)"ì´ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ ìµœëŒ€ ë‘ ë¬¸ì¥ìœ¼ë¡œ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”. ê·¸ë¦¬ê³ , MDDê°€ íˆ¬ììì—ê²Œ ì™œ ì¤‘ìš”í•œì§€ ê°„ëµíˆ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.
    2.  ì„±ê³¼ ë¹„êµ: ì‚¬ìš©ìì˜ í¬íŠ¸í´ë¦¬ì˜¤ê°€ ë²¤ì¹˜ë§ˆí¬(KOSPI) ëŒ€ë¹„ ìˆ˜ìµë¥ ê³¼ ì•ˆì •ì„±(MDD) ë©´ì—ì„œ ì–´ë• ëŠ”ì§€ í‰ê°€í•´ì£¼ì„¸ìš”.
    3.  í•µì‹¬ ì¸ì‚¬ì´íŠ¸ (ê¸°ê°„ ë°©ì–´): [3. í•˜ë½ì¥ ìƒì„¸ ë¶„ì„] ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬, ë²¤ì¹˜ë§ˆí¬ê°€ í­ë½í•˜ëŠ” 'ê¸°ê°„' ë™ì•ˆ ì±„ê¶Œê³¼ í˜„ê¸ˆì´ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ ë°©ì–´í•˜ëŠ” ë° ì–´ë–¤ ì—­í• ì„ í–ˆëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰í•´ì£¼ì„¸ìš”. (ì´ ë¶€ë¶„ì€, ë¬¸ë‹¨ì„ ë³„ë„ë¡œ ë‚˜ëˆ„ì–´ ê°•ì¡°í•´ ì£¼ì„¸ìš”)
    4.  í•µì‹¬ ì¸ì‚¬ì´íŠ¸ (í•˜ë£¨ ë°©ì–´): [4. ìµœì•…ì˜ í•˜ë£¨ ë¶„ì„] ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬, {worst_day_analysis['date']} ë‹¹ì¼ KOSPIê°€ ê¸‰ë½í–ˆì„ ë•Œ í¬íŠ¸í´ë¦¬ì˜¤ê°€ ì–¼ë§ˆë‚˜ ì˜ ë°©ì–´í–ˆëŠ”ì§€ ìˆ˜ì¹˜ë¥¼ ë¹„êµí•˜ë©° ì–¸ê¸‰í•´ì£¼ì„¸ìš”.
    """
    return prompt

# --- 4. Flask API ì„œë²„ ---

app = Flask(__name__)
CORS(app) 

@app.route('/api/backtest')
def get_backtest_data():
    """ â­ï¸ [ìˆ˜ì •] ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ + AI í”„ë¡¬í”„íŠ¸ ìºì‹œ + 'í•˜ë½ì¥ ë¶„ì„ ê²°ê³¼' ë°˜í™˜ """
    
    global g_ai_prompt_cache, g_backtest_result_cache
    
    # â­ï¸ ë°˜ë³µ ìš”ì²­ ë°©ì§€ ìºì‹œ
    if g_backtest_result_cache is not None:
        # print("ğŸ”„ ìºì‹œëœ ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.") # (ë””ë²„ê¹…ìš©)
        return jsonify(g_backtest_result_cache)
    
    print("â³ ìƒˆë¡œìš´ ë°±í…ŒìŠ¤íŠ¸ ê³„ì‚°ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    price_df = load_data(ETF_FILE, KOSPI_FILE, START_DATE, END_DATE)
    if price_df is None: return jsonify({"error": "ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 500
        
    portfolio_history_list = run_monthly_rebalancing_backtest(
        price_df, INITIAL_CAPITAL, TARGET_WEIGHTS, ASSETS_BY_GROUP
    )
    
    benchmark_series = (price_df['benchmark'] / price_df['benchmark'].iloc[0]) * INITIAL_CAPITAL
    benchmark_history_list = [
        {'date': date.strftime('%Y-%m-%d'), 'value': value}
        for date, value in benchmark_series.items()
    ]
    
    portfolio_series_for_stats = pd.Series(
        [item['value'] for item in portfolio_history_list], 
        index=pd.to_datetime([item['date'] for item in portfolio_history_list])
    )
    stats_portfolio = calculate_stats(portfolio_series_for_stats)
    stats_benchmark = calculate_stats(benchmark_series)
    
    # AI í”„ë¡¬í”„íŠ¸ ìƒì„± (ë° í•˜ë½ì¥ ë¶„ì„ ë°ì´í„° ìƒì„±)
    user_period_analyses = [] # â­ï¸ AI ë¶„ì„ í•¨ìˆ˜ê°€ ì‹¤íŒ¨í•  ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì´ˆê¸°í™”
    try:
        mdd_period_analysis, worst_day_analysis, user_period_analyses = find_analysis_data(
            portfolio_history_list, benchmark_history_list, CRASH_PERIODS
        )
        g_ai_prompt_cache = generate_ai_analysis_prompt(
            {"portfolio": stats_portfolio, "benchmark": stats_benchmark}, 
            mdd_period_analysis, 
            worst_day_analysis, 
            user_period_analyses,
            TARGET_WEIGHTS
        )
        print("âœ… AI í”„ë¡¬í”„íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ìºì‹œë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ AI í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        g_ai_prompt_cache = None

    # â­ï¸ [ìˆ˜ì •] í”„ëŸ°íŠ¸ì—”ë“œì— 'í•˜ë½ì¥ ë¶„ì„ ê²°ê³¼' ë°ì´í„° ì¶”ê°€ ì „ë‹¬
    result_data = {
        "portfolio_history": portfolio_history_list,
        "benchmark_history": benchmark_history_list,
        "stats": { "portfolio": stats_portfolio, "benchmark": stats_benchmark },
        "crash_period_results": user_period_analyses # â­ï¸ AIê°€ ë¶„ì„í•œ 'ê²°ê³¼'ë¥¼ ì „ë‹¬
    }
    
    g_backtest_result_cache = result_data # â­ï¸ ê²°ê³¼ ìºì‹œ
    return jsonify(result_data)

@app.route('/api/analyze_stream')
def analyze_stream():
    """ â­ï¸ AIì˜ ë‹µë³€ì„ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°(SSE)í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸ """

    def stream_analysis():
        global g_ai_prompt_cache
        
        if not model:
            yield f"data: {json.dumps({'text': 'ì˜¤ë¥˜: AI ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.'})}\n\n"
            yield f"data: {json.dumps({'event': 'done'})}\n\n"
            return
            
        if not g_ai_prompt_cache:
            yield f"data: {json.dumps({'text': 'ì˜¤ë¥˜: AI ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ê°€ ìºì‹œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. /api/backtestë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.'})}\n\n"
            yield f"data: {json.dumps({'event': 'done'})}\n\n"
            return

        try:
            response = model.generate_content(g_ai_prompt_cache, stream=True)
            for chunk in response:
                if chunk.text:
                    yield f"data: {json.dumps({'text': chunk.text})}\n\n"
                    time.sleep(0.02)
            yield f"data: {json.dumps({'event': 'done'})}\n\n"
            
        except Exception as e:
            print(f"âŒ AI ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜: {e}")
            yield f"data: {json.dumps({'text': f'AI ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}'})}\n\n"
            yield f"data: {json.dumps({'event': 'done'})}\n\n"

    return Response(stream_analysis(), mimetype='text/event-stream')


if __name__ == '__main__':
    # â­ï¸ debug=Falseë¡œ ë³€ê²½í•˜ì—¬ ìë™ ìƒˆë¡œê³ ì¹¨ ë°©ì§€
    app.run(debug=False, host='0.0.0.0', port=5000)